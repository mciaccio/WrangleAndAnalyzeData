{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tweepy \n",
    "import json\n",
    "import pprint as pp\n",
    "import sys\n",
    "import time\n",
    "import collections\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open two saved pandas DataFrames.  \n",
    "<br>\n",
    "The twitter_archive_common_df and the image_common_df have the same tweet_id(s).  \n",
    "This was done in preparation for the __tweet_id__ mediated merge of these two DataFrames with the DataFrame Gathered using the tweepy.API. The following Gather process creates the 3rd required pandas DataFrame \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tweet_id() are common in both DataFrames\n",
    "* image_common_df\n",
    "* twitter_archive_common_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_common_df.shape\n",
      "(1982, 12)\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the image data, instantiate the image_common_df pandas DataFrame\n",
    "with open('image_common_df.pkl', 'rb') as f:\n",
    "    image_common_df = pickle.load(f)\n",
    "    \n",
    "print('image_common_df.shape')\n",
    "print(image_common_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "twitter_archive_common_df.shape\n",
      "(1982, 11)\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the twitter_archive data, \n",
    "# instantiate the twitter_archive_common_df pandas DataFrame\n",
    "\n",
    "with open('twitter_archive_common_df.pkl', 'rb') as f:\n",
    "    twitter_archive_common_df = pickle.load(f)\n",
    "    \n",
    "print('twitter_archive_common_df.shape')\n",
    "print(twitter_archive_common_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(common_tweet_id_list) - 1982\n"
     ]
    }
   ],
   "source": [
    "# Create tweet_id driver list for tweepy.API\n",
    "common_tweet_id_list = twitter_archive_common_df.tweet_id.tolist()\n",
    "print('len(common_tweet_id_list) - {}'.format(len(common_tweet_id_list)))\n",
    "\n",
    "# development / testing overrides\n",
    "# common_tweet_id_list = [754011816964026368,666020888022790149]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirements compliance  \n",
    "Data Analyst Nanodegree Program  \n",
    "8\\. Data Wrangling  \n",
    "Project: Wrangle and Analyze Data  \n",
    "4\\. Twitter API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preparation for Tidy driven pandas Series tweet_id merge of 3 pandas DataFrames\n",
    "* pandas tweet_id Series must have the\n",
    "same content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capture missing, unmatched tweet_id(s)  \n",
    "<br>\n",
    "Requirements compliance  \n",
    "* After querying each tweet ID, you will write its JSON data to the required tweet_json.txt file with each tweet's JSON data on its own line. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter - 100\n",
      "counter - 200\n",
      "counter - 300\n",
      "counter - 400\n",
      "counter - 500\n",
      "counter - 600\n",
      "tweet_id - 754011816964026368\n",
      "tweep_error - [{'code': 144, 'message': 'No status found with that ID.'}]\n",
      "tweep_error.api_code - 144\n",
      "tweep_error.response - <Response [404]>\n",
      "type(tweep_error.response) - <class 'requests.models.Response'>\n",
      "\n",
      "counter - 700\n",
      "counter - 800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter - 900\n",
      "tweet_id - 718613305783398402\n",
      "tweep_error - Failed to send request: ('Connection aborted.', OSError(\"(60, 'ETIMEDOUT')\",))\n",
      "tweep_error.api_code - None\n",
      "tweep_error.response - None\n",
      "type(tweep_error.response) - <class 'NoneType'>\n",
      "\n",
      "counter - 1000\n",
      "counter - 1100\n",
      "counter - 1200\n",
      "counter - 1300\n",
      "counter - 1400\n",
      "counter - 1500\n",
      "counter - 1600\n",
      "counter - 1700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter - 1800\n",
      "tweet_id - 669353438988365824\n",
      "tweep_error - Failed to send request: ('Connection aborted.', OSError(\"(60, 'ETIMEDOUT')\",))\n",
      "tweep_error.api_code - None\n",
      "tweep_error.response - None\n",
      "type(tweep_error.response) - <class 'NoneType'>\n",
      "\n",
      "counter - 1900\n"
     ]
    }
   ],
   "source": [
    "# Real time tweepy.API gets \n",
    "# On the fly update tweet_json.txt\n",
    "counter = 0\n",
    "tweet_id_error_dictionary = {}\n",
    "tweet_id_error_list = []\n",
    "\n",
    "auth = tweepy.OAuthHandler('cZFGyPnfKa2ezCMqNMqSDYLEY', 'EMMU1n2TU415x15Q8UPD58P5hfBPAu1HeZWulF6xFEVjiRn40a')\n",
    "auth.set_access_token('168897950-kHN9qymcxBKdxO6XtrncugxGqXxWibKecwpDGYCR',\n",
    "                      'oHt0QgobyfWmQfUuHwPPBklQefHClsmzbiaROkUP8oYvc')\n",
    "\n",
    "api = tweepy.API(auth, wait_on_rate_limit = True, wait_on_rate_limit_notify = True)\n",
    "\n",
    "with open('tweet_json.txt', 'w') as file:\n",
    "    for tweet_id in common_tweet_id_list:\n",
    "        # print('tweet_id - {}'.format(tweet_id))\n",
    "     \n",
    "        attempts = 0\n",
    "        counter +=1\n",
    "        \n",
    "        # mitigate sporious network errors\n",
    "        # see Python try - except clauses below\n",
    "        while attempts < 5: \n",
    "            attempts += 1 \n",
    "            # populate tweet_json.txt\n",
    "            try:\n",
    "                tweepy_status = api.get_status(str(tweet_id), tweet_mode='extended')\n",
    "                # print('tweepy_status - {}\\n'.format(tweepy_status))\n",
    "                #\n",
    "                # tweepy_status - \n",
    "                # Status(_api=<tweepy.api.API object at 0x1136f3e80>,\n",
    "                # _json={'created_at': \n",
    "                # 'Tue Aug 01 16:23:56 +0000 2017', \n",
    "                # 'id': 892420643555336193, \n",
    "                # 'id_str': '892420643555336193', \n",
    "                # 'full_text': \"This is Phineas. ...\n",
    "\n",
    "                # print('type(tweepy_status) - {}\\n'.format(type(tweepy_status)))\n",
    "                #       type(tweepy_status) - <class 'tweepy.models.Status'>\n",
    "\n",
    "                json.dump(tweepy_status._json, file)\n",
    "                file.write('\\n')\n",
    "                break\n",
    "            # Capture missing tweet_id(s)\n",
    "            except tweepy.TweepError as tweep_error:\n",
    "                print('tweet_id - {}'.format(tweet_id))\n",
    "                print('tweep_error - {}'.format(tweep_error))\n",
    "                print('tweep_error.api_code - {}'.format(tweep_error.api_code))\n",
    "                print('tweep_error.response - {}'.format(tweep_error.response))\n",
    "                print('type(tweep_error.response) - {}\\n'.format(type(tweep_error.response)))\n",
    "            \n",
    "                if tweep_error.response is not None: \n",
    "                    temp_list = tweep_error.response.text.split(',')\n",
    "            \n",
    "                    for my_string in temp_list:\n",
    "                        # \"message\":\"No status found with that ID.\"}]}\n",
    "                        # print('my_string')\n",
    "                        # print(my_string)\n",
    "                        # print()\n",
    "                        if 'message' in my_string and \\\n",
    "                        'No status found with that ID.' in my_string:\n",
    "                            tweet_id_error_dictionary['tweet_id'] = tweet_id\n",
    "                            tweet_id_error_dictionary['code'] = tweep_error.api_code\n",
    "                            tweet_id_error_dictionary['message'] = my_string\n",
    "            \n",
    "                            tweet_id_error_list.append(tweet_id_error_dictionary)\n",
    "                \n",
    "                            tweet_id_error_dictionary = {}\n",
    "                    break\n",
    "                    \n",
    "            # mitigate sporious network errors\n",
    "            except Exception as err:\n",
    "                print('Exception - sys.exc_info()') \n",
    "                print(sys.exc_info())\n",
    "                print('tweet_id - {}'.format(tweet_id))\n",
    "                print('counter - {}'.format(counter))\n",
    "                print(\"err {}\".format(err)) \n",
    "                print()\n",
    "\n",
    "        if counter % 100 == 0:\n",
    "            print('counter - {}'.format(counter))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tweet_id_error_list\n",
      "[{'tweet_id': '754011816964026368', 'code': 144, 'message': '\"message\":\"No status found with that ID.\"}]}'}]\n",
      "\n",
      "len(tweet_id_error_list)\n",
      "1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\ntweet_id_error_list')\n",
    "print(tweet_id_error_list)\n",
    "print()\n",
    "\n",
    "print('len(tweet_id_error_list)')\n",
    "print(len(tweet_id_error_list))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success  \n",
    "Missing tweet_id(s) captured"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "took an hour plus, make sure tweet_json.txt is there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.isfile('tweet_json.txt') \\\n",
    "and os.path.getsize('tweet_json.txt') > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success  \n",
    "Requirements compliance  \n",
    "\"you will write its JSON data to the required tweet_json.txt file\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet_id_error_list\n",
      "[{'tweet_id': '754011816964026368', 'code': 144, 'message': '\"message\":\"No status found with that ID.\"}]}'}]\n"
     ]
    }
   ],
   "source": [
    "# Save - took hour plus processing time\n",
    "with open('tweet_id_error_list.pkl', 'wb') as f:\n",
    "    pickle.dump(tweet_id_error_list, f)\n",
    "\n",
    "# READ - make sure we can read it\n",
    "with open('tweet_id_error_list.pkl', 'rb') as f:\n",
    "    tweet_id_error_list = pickle.load(f)\n",
    "\n",
    "print('tweet_id_error_list')\n",
    "print(tweet_id_error_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirements compliance  \n",
    "Data Analyst Nanodegree Program  \n",
    "8\\. Data Wrangling  \n",
    "Project: Wrangle and Analyze Data  \n",
    "4\\. Twitter API  \n",
    "\n",
    "You will then read this file, line by line, ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(tweet_data_list) - 1981\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# populate tweet_data_list from the saved tweet_json.txt  \n",
    " \n",
    "tweet_data_list = []\n",
    "with open('tweet_json.txt', 'r') as json_file:\n",
    "    for tweet_data in json_file:\n",
    "        if tweet_data.strip() != '':\n",
    "            tweet_data_list.append(tweet_data)\n",
    "            \n",
    "print('len(tweet_data_list) - {}\\n'.format(len(tweet_data_list)))\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirements compliance.   \n",
    "Data Analyst Nanodegree Program  \n",
    "8\\. Data Wrangling  \n",
    "Project: Wrangle and Analyze Data  \n",
    "4\\. Twitter API  \n",
    "\n",
    "... to create a pandas DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create pandas DataFrame from tweet_json.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_tweet_dictionary = {}\n",
    "\n",
    "# instantiate new retrieved tweet pandas DataFrame \n",
    "retrieved_tweet_data_df = pd.DataFrame\\\n",
    "(columns = ['tweet_id','retweet_count','favorite_count'])\n",
    "\n",
    "for tweet_data in tweet_data_list:\n",
    "    # populate retrieved_tweet_dictionary with retrieved tweet_data\n",
    "    retrieved_tweet_dictionary = json.loads(tweet_data)\n",
    "    # print('retrieved_tweet_dictionary - {}'.format(retrieved_tweet_dictionary))\n",
    "\n",
    "    # populate pandas DataFrame with retrieved tweet data \n",
    "    # tweet_id, retweet_count, favorite_count columns  \n",
    "    \n",
    "    retrieved_tweet_data_df = retrieved_tweet_data_df.append\\\n",
    "    ({'tweet_id':retrieved_tweet_dictionary['id'],\\\n",
    "    'retweet_count':retrieved_tweet_dictionary['retweet_count'],\n",
    "    'favorite_count':retrieved_tweet_dictionary['favorite_count']}, \\\n",
    "     ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieved_tweet_data_df.shape - (1981, 3)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('retrieved_tweet_data_df.shape - {}\\n'.\\\n",
    "      format(retrieved_tweet_data_df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success  \n",
    "Requirements compliance  \n",
    "retrieved_tweet_data_df pandas DataFrame created from tweet_json.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in preparation for Tidy driven tweet_id mediated pd.merge of 3 DataFrames  \n",
    "pandas tweet_id Series must have the same content "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "synchronize pandas tweet_id Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet_id_to_be_removed_list\n",
      "['754011816964026368']\n",
      "\n",
      "twitter_archive_common_df.shape\n",
      "(1982, 11)\n",
      "\n",
      "image_common_df.shape\n",
      "(1982, 12)\n",
      "\n",
      "retrieved_tweet_data_df.shape\n",
      "(1981, 3)\n",
      "\n",
      "tweet_id - 754011816964026368\n",
      "\n",
      "twitter_archive_common_df.shape\n",
      "(1981, 11)\n",
      "\n",
      "image_common_df.shape\n",
      "(1981, 12)\n",
      "\n",
      "retrieved_tweet_data_df.shape\n",
      "(1981, 3)\n",
      "\n",
      "len(retrieved_tweet_id_as_str) - 1981\n"
     ]
    }
   ],
   "source": [
    "tweet_id_to_be_removed_list = []\n",
    "\n",
    "for error_tweet in tweet_id_error_list:\n",
    "    tweet_id_to_be_removed_list.append(error_tweet.get('tweet_id'))\n",
    "    \n",
    "print('tweet_id_to_be_removed_list')\n",
    "print(tweet_id_to_be_removed_list)\n",
    "print()\n",
    "\n",
    "print('twitter_archive_common_df.shape')\n",
    "print(twitter_archive_common_df.shape)\n",
    "print()\n",
    "\n",
    "print('image_common_df.shape')\n",
    "print(image_common_df.shape)\n",
    "print()\n",
    "\n",
    "print('retrieved_tweet_data_df.shape')\n",
    "print(retrieved_tweet_data_df.shape)\n",
    "print()\n",
    "\n",
    "for tweet_id in tweet_id_to_be_removed_list:\n",
    "    print('tweet_id - {}'.format(tweet_id))\n",
    "    image_common_df = image_common_df[image_common_df.tweet_id != tweet_id]\n",
    "    twitter_archive_common_df = \\\n",
    "    twitter_archive_common_df[twitter_archive_common_df.tweet_id != tweet_id]\n",
    "    \n",
    "print()\n",
    "        \n",
    "print('twitter_archive_common_df.shape')\n",
    "print(twitter_archive_common_df.shape)\n",
    "print()\n",
    "\n",
    "print('image_common_df.shape')\n",
    "print(image_common_df.shape)\n",
    "print()\n",
    "\n",
    "print('retrieved_tweet_data_df.shape')\n",
    "print(retrieved_tweet_data_df.shape)\n",
    "print()\n",
    "\n",
    "twitter_archive_tweet_id_list = \\\n",
    "twitter_archive_common_df.tweet_id.tolist()\n",
    "\n",
    "image_common_df_tweet_id_list =  image_common_df.tweet_id.tolist()\n",
    "# hack to get the quotes put around each tweet_id\n",
    "retrieved_tweet_id_as_str = [str(i) for i in retrieved_tweet_data_df.tweet_id.tolist()] \n",
    "print('len(retrieved_tweet_id_as_str) - {}'.format(len(retrieved_tweet_id_as_str)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(collections.Counter(twitter_archive_tweet_id_list) == \\\n",
    "      collections.Counter(image_common_df_tweet_id_list) ==\\\n",
    "      collections.Counter(retrieved_tweet_id_as_str))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success  \n",
    "pandas Series tweet_id synchronized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_common_df.shape\n",
      "(1981, 12)\n"
     ]
    }
   ],
   "source": [
    "# Save\n",
    "with open('image_common_df.pkl', 'wb') as f:\n",
    "    pickle.dump(image_common_df, f)\n",
    "\n",
    "# READ\n",
    "with open('image_common_df.pkl', 'rb') as f:\n",
    "    image_common_df = pickle.load(f)\n",
    "\n",
    "print('image_common_df.shape')\n",
    "print(image_common_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "twitter_archive_common_df.shape\n",
      "(1981, 11)\n"
     ]
    }
   ],
   "source": [
    "# Save\n",
    "with open('twitter_archive_common_df.pkl', 'wb') as f:\n",
    "    pickle.dump(twitter_archive_common_df, f)\n",
    "\n",
    "# READ\n",
    "with open('twitter_archive_common_df.pkl', 'rb') as f:\n",
    "    twitter_archive_common_df = pickle.load(f)\n",
    "\n",
    "print('twitter_archive_common_df.shape')\n",
    "print(twitter_archive_common_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieved_tweet_data_df.shape\n",
      "(1981, 3)\n"
     ]
    }
   ],
   "source": [
    "# Save\n",
    "with open('retrieved_tweet_data_df.pkl', 'wb') as f:\n",
    "    pickle.dump(retrieved_tweet_data_df, f)\n",
    "\n",
    "# READ\n",
    "with open('retrieved_tweet_data_df.pkl', 'rb') as f:\n",
    "    retrieved_tweet_data_df = pickle.load(f)\n",
    "\n",
    "print('retrieved_tweet_data_df.shape')\n",
    "print(retrieved_tweet_data_df.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
